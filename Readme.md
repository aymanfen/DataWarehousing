# ðŸ“¦ LLM Evaluation Data Warehousing & Analytics

## ðŸ“Œ Project Summary  
This project leverages the **Microsoft BI stack (SQL Server, SSIS, SSAS, Power BI)** to build a **complete analytical data warehousing solution** for **LLM (Large Language Model) evaluation data**. The goal is to centralize benchmark results, enable multidimensional analysis, and support data-driven decisions regarding **model performance**, **resource consumption**, and **environmental sustainability**.

---

## ðŸ§° Technology Stack  

| Layer     | Technology         |
|--------   |--------------------|
| Database  | SQL Server (DW)    |
| ETL       | SSIS               |
| OLAP      | SSAS               |
| BI        | Microsoft Power BI |

---

## ðŸŽ¯ Analytical Objective  

The objective is to **benchmark and analyse LLM performance versus cost and environmental impact**, using KPIs that can be explored **across multiple analytical dimensions**.

### ðŸ“ˆ Core KPIs  

| Metric            | Description                                    |
|-------------------|------------------------------------------------|
| **Eval Value**    | Primary benchmark score from evaluation engine |
| **Params (M)**    | Total model parameter count                    |
| **COâ‚‚ Cost (kg)** | Estimated carbon emissions                     |

### ðŸ§© Required Analytical Slicing Dimensions  
The cube must support slicing, filtering, and drill-down across **three perspectives**:

1. **Time** â€” Day â†’ Month â†’ Quarter â†’ Year  
2. **Model Attributes** â€” architecture, license, precision, hardware, etc.  
3. **Benchmark Type** â€” benchmark suite, category, or dataset  

---

## ðŸ—„ï¸ Data Modeling  

This project employs a **classic dimensional modeling approach**:  

- **Star Schema** for analytical reporting  
- **Multidimensional Cube (SSAS)** for OLAP exploration  

![starschema](./Misc/starschema.jpg)

---

## ðŸ” ETL Architecture (SSIS)


1ï¸âƒ£ **High-Level ETL Flow â€” conceptual data movement**
![HighLevelETL](./Misc/control_flow.jpg)

2ï¸âƒ£ **BenchmarkDim**

![BenchmarkDim](./Misc/dim_benchmark.jpg)

3- **ModelDim**

![ModelDim](./Misc/dim_model.jpg)

4- **FactTable**

![FactTable](./Misc/fact_flow.jpg)


---

## ðŸ§® OLAP Cube Architecture (SSAS)

### ðŸ“Š Dimension Design  

| Dimension     | Key Elements                            |
|---------------|-----------------------------------------|
| **Date**      | Hierarchy: Year â†’ Quarter â†’ Month â†’ Day |
| **Model**     | Surrogate ModelKey + attributes         |
| **Benchmark** | Category â†’ BenchmarkName                |

### ðŸ§© Measures & Calculation Logic  

| Measure              | Formula                        | Aggregation |
|----------------------|--------------------------------|-------------|
| **Total COâ‚‚ Cost**   | `SUM(FactEval[Co2Cost])`       | Sum         |
| **Avg Eval Value**   | `AVERAGE(FactEval[EvalValue])` | Average     |
| **Total Params (M)** | `SUM(FactEval[Params])`        | Sum         |

---

## ðŸ“Š Dashboard Overview  
A Power BI dashboard is built to explore model performance, cost, and trade-offs interactively.

![Dashboard](./Misc/dashboard.jpg)

Recommended views:
- Eval Value vs COâ‚‚ Cost correlation  
- Model size heatmap  
- Time-series evolution of benchmark performance  
- Architecture-based comparison slicer  

---

## ðŸ” Main Insights  

**High-accuracy models (Eval Value > 600) often incur 2â€“3Ã— more COâ‚‚ than mid-range models**, suggesting diminishing returns at the high end.  

**Certain lightweight architectures achieve competitive performance at â‰¤10M parameters**, revealing opportunities for sustainable deployment.  

**Benchmark slicing exposes generalist vs specialist models:**  
   - Generalists maintain stable scores across benchmarks  
   - Specialists excel only on specific datasets  

